<!DOCTYPE html><html><head>
      <title>template_structure</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\andre\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.8.10\crossnote\dependencies\katex\katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="navigating-the-linux-landscape-a-feasibility-study-on-employing-llama-v2-for-crafting-a-context-aware-virtual-assistant-tailored-for-arch-linux-users">Navigating the Linux Landscape: A Feasibility Study on Employing LLaMa v2 for Crafting a Context-Aware Virtual Assistant Tailored for Arch Linux Users </h1>
<h2 id="abstract">Abstract </h2>
<p>The increasing complexity of Linux systems, coupled with the growing demand for automation in software development and system management, necessitates innovative solutions [15, 16]. This research explores the feasibility of employing a Large Language Model [11, 13], specifically LLaMa v2, to create a cost-effective Virtual Assistant [7, 8, 10, 11, 13] tailored for Arch Linux users. The aim is to enhance user experience by providing context-specific suggestions and addressing general queries related to the installed software.</p>
<h4 id="keywords">Keywords </h4>
<p>GNU/Linux, Large Language Models, Virtual Assistant, Automatization, Natural Language Process</p>
<h2 id="introduction">Introduction </h2>
<p>As the landscape of Linux systems evolves, the intricate nature of these platforms poses challenges for users, particularly in terms of system management and software development. The versatility and continuous evolution of GNU/Linux, in alignment with its open-source philosophy, contribute to a dynamic environment marked by diverse distributions and frequent updates [15, 16, 18]. In this context, the demand for effective automation tools becomes becomes essential to streamline tasks and improve user productivity [17, 18].</p>
<p>Recognizing the need for innovative solutions in this complex Linux ecosystem, this research delves into the potential of leveraging advanced technologies to address the challenges faced by Arch Linux users [15]. With a focus on system management and software-related queries, the study seeks to explore the feasibility of employing a Large Language Model (LLM) known as LLaMa v2 [11, 13]. LLMs, characterized by their natural language processing capabilities, offer a promising path to creating intelligent and cost-effective virtual assistants [7, 8, 10, 11, 13].</p>
<h3 id="problem-context">Problem context </h3>
<p>The challenges inherent in managing Arch Linux systems are amplified by the system's rolling release model, introducing potential stability issues [15]. Moreover, the unique philosophy of GNU/Linux, where information is distributed across a multitude of sources without a centralized repository, presents a formidable challenge for developers seeking to generalize information for broader use [15, 16, 18]. The dynamism of GNU/Linux distributions, with new ones emerging and others becoming obsolete in short periods, further complicates the process of collecting and maintaining up-to-date data [17, 18].</p>
<p>Recognizing these complexities, this research seeks to navigate the intricate Linux landscape by proposing a Virtual Assistant tailored for Arch Linux users [15]. The Virtual Assistant aims to empower users with context-specific suggestions and solutions to general queries about their installed software, thereby improving the overall user experience [15, 16, 18].</p>
<h3 id="research-gap-and-motivation">Research gap and motivation </h3>
<p>While Virtual Assistants have become increasingly common [1-5], their adaptation to the nuances of the Linux environment, especially tailored for specific distributions like Arch Linux, remains an underexplored domain. The potential benefits of employing LLaMa v2 as the backbone for such a Virtual Assistant further motivate this investigation, with the goal of contributing to the efficiency and user-friendliness of Linux system management and software-related tasks [14].</p>
<p>By addressing the unique challenges presented by Arch Linux and the broader GNU/Linux ecosystem, this research aspires to pave the way for the development of intelligent and adaptive Virtual Assistants, fostering a more seamless integration of automation in the Linux user experience [6].</p>
<h3 id="research-question">Research question </h3>
<p>Is it feasible to utilize LLaMa v2 to construct a cost-efficient Virtual Assistant capable of assisting Arch Linux users in managing their operating systems, providing context-specific suggestions, and addressing general software-related inquiries?</p>
<h3 id="hypothesis">Hypothesis </h3>
<p>The integration of LLaMa v2 into a Virtual Assistant for Arch Linux users has the potential to significantly enhance system management and user experience, offering valuable insights and automated solutions.</p>
<h4 id="objectives">Objectives </h4>
<ul>
<li>Develop a Virtual Assistant utilizing pre-trained LLaMa v2.</li>
<li>Provide general information about the current state of the user's operating system.</li>
<li>Offer context-specific suggestions based on the installed software.</li>
<li>Address specific inquiries related to the Arch Linux operating system.</li>
</ul>
<h3 id="methodology-still-under-construction">Methodology (Still under construction) </h3>
<ol>
<li>Pre-training the LLaMa v2 Model:</li>
</ol>
<ul>
<li>
<p>Data Collection:</p>
<ul>
<li>Gather a diverse and representative dataset from the Arch Linux wiki, man pages, and Ubuntu's wiki.</li>
<li>Ensure the dataset covers a wide range of topics related to Linux systems, software, and operations.</li>
</ul>
</li>
<li>
<p>Pre-processing:</p>
<ul>
<li>Clean and preprocess the collected data to remove irrelevant information, noise, or duplicate content. To aim this purpose a shell (Bash) script must be created</li>
<li>Tokenize and structure the data to facilitate effective learning.</li>
</ul>
</li>
<li>
<p>Pre-training:</p>
<ul>
<li>Utilize the pre-processed dataset to pre-train the LLaMa v2 model.</li>
<li>Leverage transfer learning to enhance the model's understanding of Linux-specific language and context.</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Fine-tuning for Individual Users using LSTM:</li>
</ol>
<ul>
<li>
<p>User-Specific Data:</p>
<ul>
<li>Collect user-specific data, possibly including historical commands, preferences, and interactions with the Virtual Assistant.</li>
</ul>
</li>
<li>
<p>Fine-tuning Process:</p>
<ul>
<li>Fine-tune the pre-trained LLaMa v2 model for each individual user using Long Short-Term Memory (LSTM) networks.</li>
<li>This fine-tuning process allows the model to adapt to each user's unique requirements and language nuances.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Evaluation:</li>
</ol>
<ul>
<li>
<p>Performance Metrics:</p>
<ul>
<li>Establish metrics to evaluate the performance of the Virtual Assistant, such as response accuracy, contextual relevance, and user satisfaction.</li>
<li>Consider using user feedback and interactions to iteratively improve the model.</li>
</ul>
</li>
<li>
<p>Testing:</p>
<ul>
<li>Conduct rigorous testing with a diverse set of Arch Linux users to validate the effectiveness of the fine-tuned model.</li>
<li>Compare the performance of the model before and after fine-tuning to measure improvements.</li>
</ul>
</li>
</ul>
<ol start="4">
<li>Iterative Improvement:</li>
</ol>
<ul>
<li>Feedback Loop:
<ul>
<li>Implement a feedback loop where user interactions contribute to continuous improvement.</li>
<li>Regularly update the model based on user feedback and evolving user needs.</li>
</ul>
</li>
</ul>
<h2 id="references">References </h2>
<p>[1] T. J. Swamy, M. Nandini, N. B, V. Karthika K, V. L. Anvitha and C. Sunitha, "Voice and Gesture based Virtual Desktop Assistant for Physically Challenged People," 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI), Tirunelveli, India, 2022, pp. 222-226, doi: 10.1109/ICOEI53556.2022.9776746.  Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/9776746">https://ieeexplore.proxyutp.elogim.com/document/9776746</a></p>
<p>[2] S. Tjayadi and V. C. Mawardi, "Laptop Recommendation Intelligent Virtual Assistant using Recurrent Neural Network with RPA for Data Scraping," 2022 IEEE 7th International Conference on Information Technology and Digital Applications (ICITDA), Yogyakarta, Indonesia, 2022, pp. 1-6, doi: 10.1109/ICITDA55840.2022.9971263.  Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/9971263">https://ieeexplore.proxyutp.elogim.com/document/9971263</a></p>
<p>[3] H. Mauny, D. Panchal, M. Bhavsar and N. Shah, "A prototype of smart virtual assistant integrated with automation," 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA), Coimbatore, India, 2021, pp. 952-957, doi: 10.1109/ICIRCA51532.2021.9544101.  Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/9544101">https://ieeexplore.proxyutp.elogim.com/document/9544101</a></p>
<p>[4] J. Leung, Z. Shen and C. Miao, "Goal-Oriented Modelling for Virtual Assistants," 2019 IEEE International Conference on Agents (ICA), Jinan, China, 2019, pp. 73-76, doi: 10.1109/AGENTS.2019.8929177.  Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/8929177">https://ieeexplore.proxyutp.elogim.com/document/8929177</a></p>
<p>[5] Hoy, Matthew B, "Alexa, Siri, Cortana, and More: An Introduction to Voice Assistants", 2018 Medical Reference Services QuarterlyVolume 37, Issue 1, Pages 81 - 88 doi: 10.1080/02763869.2018.1404391. Ref: <a href="https://scopus.proxyutp.elogim.com/record/display.uri?eid=2-s2.0-85041199121&amp;origin=reflist">https://scopus.proxyutp.elogim.com/record/display.uri?eid=2-s2.0-85041199121&amp;origin=reflist</a></p>
<p>[6] Junwen Zheng, Martin Fischer, Dynamic prompt-based virtual assistant framework for BIM information search, Automation in Construction, Volume 155, 2023, 105067, ISSN 0926-5805, <a href="https://doi.proxyutp.elogim.com/10.1016/j.autcon.2023.105067">https://doi.proxyutp.elogim.com/10.1016/j.autcon.2023.105067</a>.  Ref: <a href="https://sciencedirect.proxyutp.elogim.com/science/article/pii/S0926580523003278">https://sciencedirect.proxyutp.elogim.com/science/article/pii/S0926580523003278</a></p>
<p>[7] T. Liu, Q. Xiong and S. Zhang, "When to Use Large Language Model: Upper Bound Analysis of BM25 Algorithms in Reading Comprehension Task," 2023 5th International Conference on Natural Language Processing (ICNLP), Guangzhou, China, 2023, pp. 1-4, doi: 10.1109/ICNLP58431.2023.00049. Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/10236655">https://ieeexplore.proxyutp.elogim.com/document/10236655</a></p>
<p>[8] S. Marukatat, "Text generation by probabilistic suffix tree language model," 2021 16th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP), Ayutthaya, Thailand, 2021, pp. 1-4, doi: 10.1109/iSAI-NLP54397.2021.9678167. Ref: <a href="https://ieeexplore.proxyutp.elogim.com/document/9678167">https://ieeexplore.proxyutp.elogim.com/document/9678167</a></p>
<p>[9] Sergei Savvov. "All you need to know to Develop using Large Language Models Explaining in simple terms the core technologies required to start developing LLM-based applications". 2023 Towards Data Science. Ref: <a href="https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc">https://towardsdatascience.com/all-you-need-to-know-to-develop-using-large-language-models-5c45708156bc</a></p>
<p>[10] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. "LoRA: Low-Rank Adaptation of Large Language Models". 2021 Cornell University, Arxiv doi: <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a> Ref: <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p>
<p>[11] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica "S-LoRA: Serving Thousands of Concurrent LoRA Adapters". 2023 Cornell University, Arxiv doi: <a href="https://doi.org/10.48550/arXiv.2311.03285">https://doi.org/10.48550/arXiv.2311.03285</a> Ref: <a href="https://arxiv.org/abs/2311.03285">https://arxiv.org/abs/2311.03285</a></p>
<p>[12] Seán Clarke, Dan Milmo, Garry Blight "How AI chatbots like ChatGPT or Bard work – visual explainer" 2023 The guardian Ref: <a href="https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer">https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer</a></p>
<p>[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". 2019 Cornell University, Arxiv doi: <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a> Ref: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<p>[14] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le "LaMDA: Language Models for Dialog Applications". 2023  Cornell University, Arxiv doi:<br>
<a href="https://doi.org/10.48550/arXiv.2201.08239">https://doi.org/10.48550/arXiv.2201.08239</a> Ref: <a href="https://arxiv.org/pdf/2201.08239.pdf">https://arxiv.org/pdf/2201.08239.pdf</a></p>
<p>[15] Arch Linux. Arch Linux Wiki. Retrieved from: <a href="https://wiki.archlinux.org/title/Arch_Linux">https://wiki.archlinux.org/title/Arch_Linux</a></p>
<p>[16] ¿Qué es GNU/Linux?. Información sobre Debian “bookworm”. Retrieved from <a href="https://www.debian.org/releases/stable/s390x/ch01s02.es.html">https://www.debian.org/releases/stable/s390x/ch01s02.es.html</a></p>
<p>[17] Red Hat Enterprise. (2022). State of Linux in Public Cloud - Annual Review. Red Hat. Retrieved from <a href="https://www.redhat.com/rhdc/managed-files/li-state-of-linux-public-cloud-solutions-ebook-f31743-202208-en_1.pdf">https://www.redhat.com/rhdc/managed-files/li-state-of-linux-public-cloud-solutions-ebook-f31743-202208-en_1.pdf</a></p>
<p>[18] Red Hat Enterprise. (2021). Annual Linux Market Study. Red Hat. Retrieved from <a href="https://www.redhat.com/rhdc/managed-files/li-linux-market-study-ebook-f31489wg-202212-en.pdf">https://www.redhat.com/rhdc/managed-files/li-linux-market-study-ebook-f31489wg-202212-en.pdf</a></p>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>